\documentclass{article}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{amsmath,amssymb}
\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage[subpreambles=true]{standalone}
% \newcounter{example}[section] % or [subsection] if you want per-section numbering
% \renewcommand{\theexample}{\arabic{example}}


\geometry{margin=1in}

\title{AgentDS-Bench: A Benchmark/Competition for Data Science Reflecting Real-World Challenges}
\author{}
\date{}

\begin{document}

\maketitle

\section{Mission}

AgentDS-Bench is a benchmark/competition for data science that reflects real-world challenges. Human/AI exhibit their ability to address important and challenging problems in real-world domains, e.g., retail, insurance, healthcare, etc., with data science.

Real-world challenges, in our context, mean that even though our data is synthesized, the way it's generated should mirror the genuine relationships and logic found in actual real-world data. The resulting dataset should make sense, align with domain knowledge, and not contradict common sense. At the same time, the competition is designed so that achieving strong results requires participants to apply domain-specific tools/ data processing/feature engineering ((for now, we assume that all such operations must use packages available on PyPI). Simply running a generic algorithm like XGBoost without thoughtful handling of the data shouldn't be enough to reach good performance.

In addition, from our side, we have another key requirement: the data should be curated to ensure the challenge is genuinely difficult. We need to understand the theoretical upper bound of performance: if someone cheated and knew the right operations on the raw data and the exact data-generating mechanism, we should know how well they could do. At the same time, a naive or generic approach should fall short. This clear gap between naive solutions and domain-informed approaches is essential, as it highlights the real value and necessity of domain-specific data science work.

\section{Scope}

For each domain, we provide a collection of data, which might include tables, images (see Figure~\ref{fig:receipt} for an example), text (see Example~\ref{ex:claimtext1}), or any format that in reality would be used in that domain. Each domain comes with three challenges, each focused on a key, real-world supervised learning problem. Solving these challenges may require participants to work with different parts of the provided data, depending on what's most relevant for the task.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.25\linewidth]{Overall-Technical-Plan/figures/simulated_receipt.png}
    \caption{An example of receipt generated by GPT-4o}
    \label{fig:receipt}
\end{figure}

\section{Data Curation}

For data curation, we need to prepare the collection of data as raw data, and data generating mechanisms for each challenge to have the ground-truth label for prediction. To reflect real-world complexity and plausibility, we should first do literature(blogs, reports, papers, etc) review on 1. what \textbf{problems} (data science can help) are the most important and challenging in this domain. 2. what \textbf{features} often appear in this domain. (could be not just tabular but also images, texts, etc) 3. what \textbf{operations}, including some data science tools/data processing/feature engineering,  on some of the features are specific to that domain (so that we can borrow from). 4. what \textbf{relationships} between features and response would be reasonable to consider when synthesize the data.  We need to prepare documentations to explain some domain-specific jargon. In this way we make sure our data does not contradict to reality and show insights from that domain.

When curating the data, beyond mimicking reality, we also need to ensure that achieving strong results requires participants to apply domain-specific tools/ data processing/feature engineering, and simply running a generic algorithm like XGBoost without thoughtful handling of the data shouldn't be enough to reach good performance. We can always test on our data during curation to make sure such gap exists.

% (A detailed example to replace this) For example, in the insurance domain, we might provide a tabular dataset with general policyholder information such as age, gender, and other demographic details, alongside images documenting car accidents. The challenge could be to predict the annual insurance premium for each individual. We design the data so that information from the accident images, such as visible injuries or vehicle damage, is crucial for making accurate predictions. If participants rely only on the tabular data and ignore the images, their models will perform poorly. To achieve strong results, they must effectively extract relevant information from the images and incorporate it into their predictive models.

\subsection{Example: Claims Triage Data Generation Mechanism}

\include{Domains/Insurance/example_challenge1}

% We aim to simulate a realistic insurance claims dataset where each claim is categorized for triage—\textbf{Simple}, \textbf{Moderate}, or \textbf{Complex}—using both structured features and unstructured claim descriptions. This process is designed to reflect the feature interactions, label noise, and class imbalance seen in real-world insurance workflows. The numbers used in this example should be further verified with literature review on this domain.

% \textbf{Step 1: Structured Feature Generation}

% For each claim $i$, generate the following structured features:

% \begin{enumerate}
%     \item \textbf{Claim Type} ($X_{\text{claim\_type}}$):\\
%     Randomly assign one of the following categories to each claim, using realistic probabilities (for example):
%     \begin{itemize}
%         \item Fender-Bender (60\%)
%         \item Major Collision (20\%)
%         \item Theft (10\%)
%         \item Injury-Only (10\%)
%     \end{itemize}

%     \item \textbf{Reported Damage} ($X_{\text{reported\_damage}}$):\\
%     Simulate the reported dollar amount of damage as a positive number. The distribution parameters depend on claim type; for instance:
%     \begin{itemize}
%         \item Fender-Bender: $\mathrm{LogNormal}(\mu=7, \sigma=0.6)$
%         \item Major Collision: $\mathrm{LogNormal}(\mu=9, \sigma=1.0)$
%         \item Theft: $\mathrm{LogNormal}(\mu=8, \sigma=0.8)$
%         \item Injury-Only: $\mathrm{LogNormal}(\mu=8, \sigma=1.2)$
%     \end{itemize}
%     (You may adjust these parameters based on desired averages and variance.)

%     \item \textbf{Number of Parties Involved} ($X_{\text{num\_parties}}$):\\
%     Sample the number of vehicles or people involved using a small-integer distribution, such as:
%     \begin{itemize}
%         \item Fender-Bender: $\mathrm{Poisson}(\lambda=1)$ (mostly single-car accidents)
%         \item Major Collision: $\mathrm{Poisson}(\lambda=2)$ (multi-vehicle common)
%         \item Theft: always $1$
%         \item Injury-Only: $\mathrm{Poisson}(\lambda=1.2)$
%     \end{itemize}

%     \item \textbf{Additional Features (optional):}\\
%     Optionally include more features such as:
%     \begin{itemize}
%         \item \textit{Claimant History}: Number of prior claims (e.g., $\mathrm{Poisson}(\lambda=0.2)$)
%         \item \textit{Location}: Urban, Suburban, Rural (sampled according to national stats)
%         \item \textit{Policyholder Age}: Sample from a realistic age distribution (e.g., normal with mean 40, std 15, clipped at 18 and 90)
%     \end{itemize}
% \end{enumerate}

% \textbf{Step 2: Latent Injury Indicator}

% Introduce a latent binary variable $I_i$ indicating whether bodily injury occurred. This variable is not observed directly, but it influences both the claim description text and the final triage decision.

% \[
% P(I_i = 1 \mid X_{\text{claim\_type}}) =
% \begin{cases}
% p_{\text{injury, minor}} & \text{if } X_{\text{claim\_type}} = \text{Fender-Bender} \\
% p_{\text{injury, major}} & \text{if } X_{\text{claim\_type}} = \text{Major Collision} \\
% 0 & \text{if } X_{\text{claim\_type}} = \text{Theft} \\
% p_{\text{injury, injury}} & \text{if } X_{\text{claim\_type}} = \text{Injury}
% \end{cases}
% \]
% Typical values: $p_{\text{injury, minor}} = 0.3$, $p_{\text{injury, major}} = 0.8$, $p_{\text{injury, injury}} = 1.0$.

% \textbf{Step 3: Unstructured Text Feature Generation (LLM-based)}

% For each claim, generate a synthetic claim description $X_{\text{description}}$ using a large language model (LLM), such as GPT-4o-mini, as follows:

% \begin{enumerate}
%     \item \textbf{Determine content signals:}
%     \begin{itemize}
%         \item If $I_i = 1$ (injury present), ensure that the prompt to the LLM instructs inclusion of injury-related details (e.g., "driver injured", "hospitalized", "ambulance called").
%         \item If $X_{\text{reported\_damage}}$ is high or $X_{\text{num\_parties}} > 1$, the prompt should encourage mention of severe incident characteristics (e.g., "multiple vehicles", "major collision", "extensive damage").
%         \item If the claim is otherwise minor (low damage, no injury), the prompt should describe a routine or simple accident (e.g., "minor scratch", "no injuries reported").
%     \end{itemize}

% \item \textbf{Generate description using LLM:}
% \begin{itemize}
%     \item Prepare a prompt for the LLM that includes selected structured feature values (e.g., claim type, number of vehicles, or other relevant context), but clearly instructs the model to make injury status the focus of the brief description.
%     \item Example prompt: \\
%     \texttt{You are writing a short insurance claim description for a car accident.} \\
%     \texttt{Claim type: Major Collision. Number of vehicles involved: 3.} \\
%     \texttt{[If $I_i=1$:] The accident resulted in an injury.} \\
%     \texttt{[If $I_i=0$:] No injuries occurred in this accident.} \\
%     \texttt{Write one or two sentences describing the incident, focusing on whether anyone was injured. You may mention other details, but make the injury status clear.}
%     \item The LLM generates a text such as: \\
% % \begin{quote}
% % {Example 1:} \texttt{``Three vehicles were involved in a major collision. One driver suffered a minor injury and received medical care.'' }\label{ex:claimtext1} \\
% % {Example 2:} \texttt{``This was a minor accident involving two cars. No injuries were reported.''} \label{ex:claimtext2}
% % \end{quote}
% \begin{examplebox}[ex:claimtext1]{Claim Descriptions}
% \textbf{Example 1:} ``Three vehicles were involved in a major collision. One driver suffered a minor injury and received medical care.''

% \medskip

% \textbf{Example 2:} ``This was a minor accident involving two cars. No injuries were reported.''
% \end{examplebox}


% \end{itemize}

% \item \textbf{Assign generated text:}
% \begin{itemize}
%     \item Use the LLM-generated description as $X_{\text{description}}$ for this claim.
%     \item Every claim in the dataset receives a unique, concise description that reflects injury status and fits naturally with the structured data.
% \end{itemize}

% \end{enumerate}

% \textit{Note:} The triage outcome $Y_i$ is determined after text generation and uses both the structured features and $I_i$. The text only reflects the underlying incident details and latent variables, not the label $Y_i$ itself.



% \textbf{Step 4: Triage Category Assignment}

% Calculate a latent severity score $S_i$ for each claim:
% \begin{equation}\label{generating}
%  S_i = w_1 \ln(1 + X_{\text{reported\_damage}}) + w_2\, X_{\text{num\_parties}} + w_3\, \mathbb{1}(I_i = 1) + \epsilon_i
% \end{equation}

% where:
% \begin{itemize}
%     \item $\mathbb{1}(I_i = 1)$ is $1$ if injuries are present, else $0$.
%     \item $w_3$ is set high so that injuries heavily increase severity.
%     \item $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ adds label noise.
% \end{itemize}

% Assign the triage class $Y_i$ using thresholds $T_1 < T_2$:
% \[
% Y_i =
% \begin{cases}
% \text{Simple} & S_i < T_1 \\
% \text{Moderate} & T_1 \leq S_i < T_2 \\
% \text{Complex} & S_i \geq T_2
% \end{cases}
% \]
% Thresholds $T_1$ and $T_2$ are chosen to ensure realistic class imbalance (e.g., $\sim$10\% Complex).


% \subsubsection*{Summary: Internal Setup vs. Participant View}

% \textbf{Internal Setup:}
% \begin{itemize}
%     \item For each claim, we generate structured features, a latent injury indicator $I_i$, a claim description (based on $I_i$ and other features), and the triage label $Y_i$.
%     \item The latent variable $I_i$ is never released; it is used internally for generating text and labels.
% \end{itemize}

% \textbf{What Participants See:}
% \begin{itemize}
%     \item Training and test sets with tabular features and a generated claim description for every claim.
%     \item The training set includes triage labels $Y_i$; the test set does not.
%     \item Participants never see $I_i$; they must infer it from the available data.
% \end{itemize}

% \textbf{Evaluation:}
% \begin{itemize}
%     \item Submissions are evaluated on multi-class accuracy or weighted F1 for triage prediction on the test set.
%     \item We can have a theoratical upper limit (oracle performance): Make prediction with Equation \ref{generating} with no noise term.
% \end{itemize}



\section{Evaluation}

This is coupled with data curation. As the challenges in the end would be prediction tasks(supervised learning), the very basic evaluation metric would be the performance metric (accuracy, mse, etc).

To give a fuller picture of participant's capability in data science, the leaderboard reports:

\begin{itemize}
    \item \textbf{Base Performance:} Main metric (accuracy, MSE) on the standard test set.
    \item \textbf{Efficiency: } Time used for code execution. And time used from receiving the datasets to submission. (There seem to be a dilemma that if we have multi-modality, we may not be able to execute code from participants)
    \item \textbf{Transparency: } Participants are  encouraged to document what tools/procedures they took. Scored by GPT and be made on leaderboard, but not used to calculate the overall ranking.
    \item \textbf{Robustness (Noise Resilience): } Performance on a noisy version of the test set (by adding typos, perturbing numeric features, etc).
    % \item \textbf{Noise Resilience:} not clear
     %\item \textbf{Information Utilization:} Gains achieved by effectively combining both structured features and text, measured by comparing to single-modality baselines.
    % \item \textbf{Task Adaptation:} Ability to handle variations in test distribution or task setup (e.g., Different class balance, Feature shifts).
\end{itemize}



{\color{red} There could be more evaluation metrics to appear on leader board. Not certain what else we can design.}




% \section{Toolsets and Limitations}

% Toolsets, what are the things AI could not do.

% Take hierarchical statistical.

% Last level is the single tabular data getting the final observation $y = f(x_1, \ldots, x_{10})$; some of $x_i$ are latent variables from upper-level models (e.g., happiness index from some second level, from consumer behavior data, other sources). We can simulate data.

% \section{Feature Engineering from Multimodality}

% Feature engineering from multimodality? Like the "add feature"? Should make sure they are indeed something that needs to be processed to tabular data and indeed useful features in reality.

% Could also be a news/notebook style on features.

% \section{Principled Way to Test Data}

% What about all the details beyond \texttt{submission.csv}? What are the details that reflect the ability of doing data science?

% Only consider supervised learning. $X_{\text{train}}$, $y_{\text{train}}$, $X_{\text{test}}$, $y_{\text{test}}$.

% Fill in the $\hat{y}$ corresponding to MSE/$L^2$ loss? Evaluation metrics. Several data metrics. Two metrics to combine them? Not clear how to combine them?

% Same $X$ but different $y$'s or different evaluation of $y$, say zero-inflated Poisson $y$.

% A naive use of, say, XGBoost or Random Forest should not achieve a good performance in our benchmark. But theoretically, one can achieve a much better performance if he/she obtains good insight from the data/hints we provide.




\section{Implementation Plan}

Start from curating data in one/two domains (starting from insurance, retail, etc.), to ensure the data:

\begin{enumerate}[label=\arabic*.]
    \item Reflects real-world complexity and challenge (look for domain knowledge from blogs, etc.).
    \item Has some difficulty, so that a naive solution would not give good results, and with proper use of datasets (hints), the performance can improve a lot.
\end{enumerate}


\section{Timeline}

\begin{itemize}
\item \textbf{Now to June:} Pilot curation of data, testing, inviting experts to review those data.
\begin{itemize}
    \item Start curating insurance data, have a small and simple one first.
    \item Discuss and add features and details on it.
    \item Testing with it.
    \item Refine our plan based on this first experience.
    \item Refine insurance data.
    \item Curate the next domain (retail) based on experience from insurance.
    \item Refine insurance and retail data and invite expert reviewing.
    \item Data curation for another new domain.
\end{itemize}

\item \textbf{The remaining weeks in June:} Flexible, reserved for adjustments, further testing, or addressing any unexpected issues that arise. Meanwhile curate data for more domains.
\item \textbf{August:} All data ready by the end of August. Corresponding front end and back end ready.

\item \textbf{October:} Hold competition and benchmark.
\end{itemize}

\end{document}
